import streamlit as st
import httpx
from bs4 import BeautifulSoup
from datetime import datetime, date, time as dtime
from zoneinfo import ZoneInfo
import re
from concurrent.futures import ThreadPoolExecutor, as_completed

st.set_page_config(page_title="í†µì‹ ê¸°ì‚¬ ìˆ˜ì§‘ê¸°_ê²½ì°°íŒ€", layout="wide")

# === í‚¤ì›Œë“œ ì¹´í…Œê³ ë¦¬ ===
keyword_groups = {
    'ì‹œê²½': ['ì„œìš¸ê²½ì°°ì²­'],
    'ë³¸ì²­': ['ê²½ì°°ì²­'],
    'ì¢…í˜œë¶': ['ì¢…ë¡œ', 'ì¢…ì•”', 'ì„±ë¶', 'ê³ ë ¤ëŒ€', 'ì°¸ì—¬ì—°ëŒ€', 'í˜œí™”', 'ë™ëŒ€ë¬¸', 'ì¤‘ë‘',
        'ì„±ê· ê´€ëŒ€', 'í•œêµ­ì™¸ëŒ€', 'ì„œìš¸ì‹œë¦½ëŒ€', 'ê²½í¬ëŒ€', 'ê²½ì‹¤ë ¨', 'ì„œìš¸ëŒ€ë³‘ì›',
        'ë…¸ì›', 'ê°•ë¶', 'ë„ë´‰', 'ë¶ë¶€ì§€ë²•', 'ë¶ë¶€ì§€ê²€', 'ìƒê³„ë°±ë³‘ì›', 'êµ­ê°€ì¸ê¶Œìœ„ì›íšŒ'],
    'ë§ˆí¬ì¤‘ë¶€': ['ë§ˆí¬', 'ì„œëŒ€ë¬¸', 'ì„œë¶€', 'ì€í‰', 'ì„œë¶€ì§€ê²€', 'ì„œë¶€ì§€ë²•', 'ì—°ì„¸ëŒ€', 'ë°˜ë¶€íŒ¨ë²”ì£„ìˆ˜ì‚¬ëŒ€', 'ê³µê³µë²”ì£„ìˆ˜ì‚¬ëŒ€',
        'ê¸ˆìœµë²”ì£„ìˆ˜ì‚¬ëŒ€', 'ë§ˆì•½ë²”ì£„ìˆ˜ì‚¬ëŒ€', 'ì‹ ì´Œì„¸ë¸Œë€ìŠ¤ë³‘ì›', 'êµ°ì¸ê¶Œì„¼í„°', 'ì¤‘ë¶€', 'ì¤‘êµ¬', 
        'ë‚¨ëŒ€ë¬¸', 'ìš©ì‚°', 'ë™êµ­ëŒ€', 'ìˆ™ëª…ì—¬ëŒ€', 'ìˆœì²œí–¥ëŒ€ë³‘ì›'],
    'ë‚¨ë¶€ì§€ê²€ë²•': ['ë‚¨ë¶€ì§€ê²€', 'ë‚¨ë¶€ì§€ë²•'],
    'ì˜ë“±í¬ê´€ì•…': ['ì˜ë“±í¬', 'ì–‘ì²œ', 'êµ¬ë¡œ', 'ê°•ì„œ', 'ì—¬ì˜ë„ì„±ëª¨ë³‘ì›',
        'ê³ ëŒ€êµ¬ë¡œë³‘ì›', 'ê´€ì•…', 'ê¸ˆì²œ', 'ë™ì‘', 'ë°©ë°°', 'ì„œìš¸ëŒ€', 'ì¤‘ì•™ëŒ€', 'ìˆ­ì‹¤ëŒ€', 'ë³´ë¼ë§¤ë³‘ì›'],
    'ê°•ë‚¨ê´‘ì§„': ['ê°•ë‚¨', 'ì„œì´ˆ', 'ìˆ˜ì„œ', 'ì†¡íŒŒ', 'ê°•ë™', 'ì‚¼ì„±ì˜ë£Œì›', 'í˜„ëŒ€ì•„ì‚°ë³‘ì›',
        'ê°•ë‚¨ì„¸ë¸Œë€ìŠ¤ë³‘ì›', 'ê´‘ì§„', 'ì„±ë™', 'ë™ë¶€ì§€ê²€', 'ë™ë¶€ì§€ë²•', 'í•œì–‘ëŒ€', 'ê±´êµ­ëŒ€', 'ì„¸ì¢…ëŒ€']
}

st.title("ğŸ“° í†µì‹ ê¸°ì‚¬ ìˆ˜ì§‘ê¸°_ê²½ì°°íŒ€")
st.markdown("âœ… í†µì‹ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í•˜ê³  ì„ íƒí•œ í‚¤ì›Œë“œê°€ ë³¸ë¬¸ì— í¬í•¨ëœ ê¸°ì‚¬ë§Œ í•„í„°ë§í•©ë‹ˆë‹¤. ì„ íƒí•œ ê¸°ì‚¬ë§Œ ìµœí•˜ë‹¨ ë³µì‚¬ìš© ë°•ìŠ¤ì— í‘œì‹œë©ë‹ˆë‹¤. ì—…ë°ì´íŠ¸:250622")

now = datetime.now(ZoneInfo("Asia/Seoul"))
col1, col2 = st.columns(2)
with col1:
    start_date = st.date_input("ì‹œì‘ ë‚ ì§œ", value=now.date())
    start_time = st.time_input("ì‹œì‘ ì‹œê°„", value=dtime(0, 0))
with col2:
    end_date = st.date_input("ì¢…ë£Œ ë‚ ì§œ", value=now.date())
    end_time = st.time_input("ì¢…ë£Œ ì‹œê°„", value=dtime(now.hour, now.minute))

selected_groups = st.multiselect("í‚¤ì›Œë“œ ê·¸ë£¹ ì„ íƒ", options=list(keyword_groups.keys()), default=['ì‹œê²½', 'ì¢…í˜œë¶'])
selected_keywords = [kw for g in selected_groups for kw in keyword_groups[g]]

start_dt = datetime.combine(start_date, start_time).replace(tzinfo=ZoneInfo("Asia/Seoul"))
end_dt = datetime.combine(end_date, end_time).replace(tzinfo=ZoneInfo("Asia/Seoul"))

progress_placeholder = st.empty()
status_placeholder = st.empty()

# âœ… ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "articles" not in st.session_state:
    st.session_state.articles = []

def highlight_keywords(text, keywords):
    for kw in keywords:
        text = re.sub(f"({re.escape(kw)})", r'<mark style="background-color: #fffb91">\1</mark>', text)
    return text

def get_content(url, selector):
    try:
        with httpx.Client(timeout=5.0) as client:
            res = client.get(url, headers={"User-Agent": "Mozilla/5.0"})
            soup = BeautifulSoup(res.text, "html.parser")
            content = soup.select_one(selector)
            return content.get_text(separator="\n", strip=True) if content else ""
    except:
        return ""

def fetch_articles_concurrently(article_list, selector):
    results = []
    progress_bar = progress_placeholder.progress(0.0, text="ë³¸ë¬¸ ìˆ˜ì§‘ ì¤‘...")
    total = len(article_list)
    with ThreadPoolExecutor(max_workers=30) as executor:
        futures = {executor.submit(get_content, art['url'], selector): art for art in article_list}
        for i, future in enumerate(as_completed(futures)):
            art = futures[future]
            try:
                content = future.result()
                if any(kw in content for kw in selected_keywords):
                    art['content'] = content
                    results.append(art)
            except:
                continue
            progress_bar.progress((i + 1) / total, text=f"{i+1}/{total} ê¸°ì‚¬ ì²˜ë¦¬ ì™„ë£Œ")
    progress_placeholder.empty()
    return results

def parse_yonhap():
    collected, page = [], 1
    status_placeholder.info("ğŸ” [ì—°í•©ë‰´ìŠ¤] ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘ ì¤‘...")
    while True:
        url = f"https://www.yna.co.kr/news/{page}?site=navi_latest_depth01"
        res = httpx.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=5.0)
        soup = BeautifulSoup(res.text, "html.parser")
        items = soup.select("ul.list01 > li[data-cid]")
        if not items:
            break
        for item in items:
            cid = item.get("data-cid")
            title_tag = item.select_one(".title01")
            time_tag = item.select_one(".txt-time")
            if not (cid and title_tag and time_tag):
                continue
            try:
                dt = datetime.strptime(f"{start_dt.year}-{time_tag.text.strip()}", "%Y-%m-%d %H:%M").replace(tzinfo=ZoneInfo("Asia/Seoul"))
            except:
                continue
            if dt < start_dt:
                return fetch_articles_concurrently(collected, "div.story-news.article")
            if start_dt <= dt <= end_dt:
                collected.append({
                    "source": "ì—°í•©ë‰´ìŠ¤", "datetime": dt, "title": title_tag.text.strip(),
                    "url": f"https://www.yna.co.kr/view/{cid}"
                })
        page += 1
    return fetch_articles_concurrently(collected, "div.story-news.article")

def parse_newsis():
    collected, page = [], 1
    status_placeholder.info("ğŸ” [ë‰´ì‹œìŠ¤] ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘ ì¤‘...")
    while True:
        url = f"https://www.newsis.com/realnews/?cid=realnews&day=today&page={page}"
        res = httpx.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=5.0)
        soup = BeautifulSoup(res.text, "html.parser")
        items = soup.select("ul.articleList2 > li")
        if not items:
            break
        for item in items:
            title_tag = item.select_one("p.tit > a")
            time_tag = item.select_one("p.time")
            if not (title_tag and time_tag):
                continue
            title = title_tag.get_text(strip=True)
            href = title_tag.get("href", "")
            match = re.search(r"\d{4}\.\d{2}\.\d{2} \d{2}:\d{2}:\d{2}", time_tag.text)
            if not match:
                continue
            dt = datetime.strptime(match.group(), "%Y.%m.%d %H:%M:%S").replace(tzinfo=ZoneInfo("Asia/Seoul"))
            if dt < start_dt:
                return fetch_articles_concurrently(collected, "div.viewer")
            if start_dt <= dt <= end_dt:
                collected.append({
                    "source": "ë‰´ì‹œìŠ¤", "datetime": dt, "title": title,
                    "url": "https://www.newsis.com" + href
                })
        page += 1
    return fetch_articles_concurrently(collected, "div.viewer")

# âœ… ê¸°ì‚¬ ìˆ˜ì§‘ ë²„íŠ¼
if st.button("ğŸ“¥ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹œì‘"):
    status_placeholder.info("ê¸°ì‚¬ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    newsis_articles = parse_newsis()
    yonhap_articles = parse_yonhap()
    st.session_state.articles = newsis_articles + yonhap_articles
    status_placeholder.success(f"âœ… ì´ {len(st.session_state.articles)}ê±´ì˜ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")

# âœ… ê¸°ì‚¬ í‘œì‹œ ë° ì„ íƒ
selected_articles = []
articles = st.session_state.articles
if articles:
    st.subheader("ğŸ“° ê¸°ì‚¬ ë‚´ìš©")
    for i, art in enumerate(articles):
        matched_kw = [kw for kw in selected_keywords if kw in art["content"]]
        with st.expander(art["title"], expanded=False):
            is_selected = st.checkbox("ì´ ê¸°ì‚¬ ì„ íƒ", key=f"select_{i}")
            st.markdown(f"[ì›ë¬¸ ë³´ê¸°]({art['url']})")
            st.markdown(f"{art['datetime'].strftime('%Y-%m-%d %H:%M')} | í•„í„°ë§ í‚¤ì›Œë“œ: {', '.join(matched_kw)}")
            st.markdown(highlight_keywords(art['content'], matched_kw).replace("\n", "<br>"), unsafe_allow_html=True)
            if is_selected:
                selected_articles.append(art)

# âœ… ë³µì‚¬ìš© í…ìŠ¤íŠ¸ ë°•ìŠ¤
if selected_articles:
    st.subheader("ğŸ“‹ ë³µì‚¬ìš© í…ìŠ¤íŠ¸ (ì„ íƒëœ ê¸°ì‚¬ë§Œ)")
    text_block = ""
    for row in selected_articles:
        text_block += f"â–³{row['title']}\n- {row['content'].strip()}\n\n"
    st.code(text_block.strip(), language="markdown")
    st.caption("âœ… ë³µì‚¬ ë²„íŠ¼ì„ ëˆŒëŸ¬ ì„ íƒí•œ ê¸°ì‚¬ ë‚´ìš©ì„ ë³µì‚¬í•˜ì„¸ìš”.")
else:
    if articles:
        st.subheader("ğŸ“‹ ë³µì‚¬ìš© í…ìŠ¤íŠ¸ (ì„ íƒëœ ê¸°ì‚¬ ì—†ìŒ)")
        st.info("ì²´í¬ë°•ìŠ¤ë¡œ ê¸°ì‚¬ ì„ íƒ ì‹œ ì´ ì˜ì—­ì— í…ìŠ¤íŠ¸ê°€ í‘œì‹œë©ë‹ˆë‹¤.")
