import streamlit as st
import httpx
from bs4 import BeautifulSoup
from datetime import datetime, date, time as dtime
from zoneinfo import ZoneInfo
import re
import asyncio

st.set_page_config(page_title="Îâ¥Ïä§ ÌÇ§ÏõåÎìú ÏàòÏßëÍ∏∞", layout="wide")

# ‚úÖ ÌÇ§ÏõåÎìú Í∑∏Î£π Ï†ïÏùò
keyword_groups = {
    'ÏãúÍ≤Ω': ['ÏÑúÏö∏Í≤ΩÏ∞∞Ï≤≠'],
    'Î≥∏Ï≤≠': ['Í≤ΩÏ∞∞Ï≤≠'],
    'Ï¢ÖÌòúÎ∂Å': [
        'Ï¢ÖÎ°ú', 'Ï¢ÖÏïî', 'ÏÑ±Î∂Å', 'Í≥†Î†§ÎåÄ', 'Ï∞∏Ïó¨Ïó∞ÎåÄ', 'ÌòúÌôî', 'ÎèôÎåÄÎ¨∏', 'Ï§ëÎûë',
        'ÏÑ±Í∑†Í¥ÄÎåÄ', 'ÌïúÍµ≠Ïô∏ÎåÄ', 'ÏÑúÏö∏ÏãúÎ¶ΩÎåÄ', 'Í≤ΩÌù¨ÎåÄ', 'Í≤ΩÏã§Î†®', 'ÏÑúÏö∏ÎåÄÎ≥ëÏõê',
        'ÎÖ∏Ïõê', 'Í∞ïÎ∂Å', 'ÎèÑÎ¥â', 'Î∂ÅÎ∂ÄÏßÄÎ≤ï', 'Î∂ÅÎ∂ÄÏßÄÍ≤Ä', 'ÏÉÅÍ≥ÑÎ∞±Î≥ëÏõê', 'Íµ≠Í∞ÄÏù∏Í∂åÏúÑÏõêÌöå'
    ],
    'ÎßàÌè¨Ï§ëÎ∂Ä': [
        'ÎßàÌè¨', 'ÏÑúÎåÄÎ¨∏', 'ÏÑúÎ∂Ä', 'ÏùÄÌèâ', 'ÏÑúÎ∂ÄÏßÄÍ≤Ä', 'ÏÑúÎ∂ÄÏßÄÎ≤ï', 'Ïó∞ÏÑ∏ÎåÄ',
        'Ïã†Ï¥åÏÑ∏Î∏åÎûÄÏä§Î≥ëÏõê', 'Íµ∞Ïù∏Í∂åÏÑºÌÑ∞', 'Ï§ëÎ∂Ä', 'ÎÇ®ÎåÄÎ¨∏', 'Ïö©ÏÇ∞', 'ÎèôÍµ≠ÎåÄ',
        'ÏàôÎ™ÖÏó¨ÎåÄ', 'ÏàúÏ≤úÌñ•ÎåÄÎ≥ëÏõê'
    ],
    'ÏòÅÎì±Ìè¨Í¥ÄÏïÖ': [
        'ÏòÅÎì±Ìè¨', 'ÏñëÏ≤ú', 'Íµ¨Î°ú', 'Í∞ïÏÑú', 'ÎÇ®Î∂ÄÏßÄÍ≤Ä', 'ÎÇ®Î∂ÄÏßÄÎ≤ï', 'Ïó¨ÏùòÎèÑÏÑ±Î™®Î≥ëÏõê',
        'Í≥†ÎåÄÍµ¨Î°úÎ≥ëÏõê', 'Í¥ÄÏïÖ', 'Í∏àÏ≤ú', 'ÎèôÏûë', 'Î∞©Î∞∞', 'ÏÑúÏö∏ÎåÄ', 'Ï§ëÏïôÎåÄ', 'Ïà≠Ïã§ÎåÄ', 'Î≥¥ÎùºÎß§Î≥ëÏõê'
    ],
    'Í∞ïÎÇ®Í¥ëÏßÑ': [
        'Í∞ïÎÇ®', 'ÏÑúÏ¥à', 'ÏàòÏÑú', 'ÏÜ°Ìåå', 'Í∞ïÎèô', 'ÏÇºÏÑ±ÏùòÎ£åÏõê', 'ÌòÑÎåÄÏïÑÏÇ∞Î≥ëÏõê',
        'Í∞ïÎÇ®ÏÑ∏Î∏åÎûÄÏä§Î≥ëÏõê', 'Í¥ëÏßÑ', 'ÏÑ±Îèô', 'ÎèôÎ∂ÄÏßÄÍ≤Ä', 'ÎèôÎ∂ÄÏßÄÎ≤ï', 'ÌïúÏñëÎåÄ',
        'Í±¥Íµ≠ÎåÄ', 'ÏÑ∏Ï¢ÖÎåÄ'
    ]
}

# ‚úÖ UI Íµ¨ÏÑ±
st.title("\U0001F4F0 Îâ¥Ïä§ ÌÇ§ÏõåÎìú ÏàòÏßëÍ∏∞")

now = datetime.now(ZoneInfo("Asia/Seoul"))
col1, col2 = st.columns(2)
with col1:
    start_date = st.date_input("ÏãúÏûë ÎÇ†Ïßú", value=now.date())
    start_time = st.time_input("ÏãúÏûë ÏãúÍ∞Ñ", value=dtime(0, 0))
with col2:
    end_date = st.date_input("Ï¢ÖÎ£å ÎÇ†Ïßú", value=now.date())
    end_time = st.time_input("Ï¢ÖÎ£å ÏãúÍ∞Ñ", value=dtime(now.hour, now.minute))

selected_groups = st.multiselect("ÌÇ§ÏõåÎìú Í∑∏Î£π ÏÑ†ÌÉù", options=list(keyword_groups.keys()), default=['ÏãúÍ≤Ω', 'Ï¢ÖÌòúÎ∂Å'])
selected_keywords = [kw for g in selected_groups for kw in keyword_groups[g]]

start_dt = datetime.combine(start_date, start_time).replace(tzinfo=ZoneInfo("Asia/Seoul"))
end_dt = datetime.combine(end_date, end_time).replace(tzinfo=ZoneInfo("Asia/Seoul"))

progress_placeholder = st.empty()
status_placeholder = st.empty()

# ‚úÖ Î≥∏Î¨∏ ÌÇ§ÏõåÎìú Í∞ïÏ°∞
def highlight_keywords(text, keywords):
    for kw in keywords:
        text = re.sub(f"({re.escape(kw)})", r"**\\1**", text)
    return text

# ‚úÖ ÎπÑÎèôÍ∏∞ Î≥∏Î¨∏ ÏàòÏßë Ìï®Ïàò
async def get_content_async(client, url, selector):
    try:
        res = await client.get(url, headers={"User-Agent": "Mozilla/5.0"})
        soup = BeautifulSoup(res.text, "html.parser")
        content = soup.select_one(selector)
        return content.get_text(separator="\n", strip=True) if content else ""
    except:
        return ""

# ‚úÖ ÎπÑÎèôÍ∏∞ Í∏∞ÏÇ¨ Î≥ëÎ†¨ ÏàòÏßë
def fetch_articles_concurrently(article_list, selector):
    progress_bar = progress_placeholder.progress(0.0, text="Î≥∏Î¨∏ ÏàòÏßë Ï§ë...")
    total = len(article_list)

    async def collect():
        results = []
        timeout = httpx.Timeout(10.0)
        limits = httpx.Limits(max_keepalive_connections=50, max_connections=100)
        async with httpx.AsyncClient(timeout=timeout, limits=limits) as client:
            tasks = [get_content_async(client, art['url'], selector) for art in article_list]
            contents = await asyncio.gather(*tasks)
            for i, content in enumerate(contents):
                progress_bar.progress((i + 1) / total, text=f"{i+1}/{total} Í∏∞ÏÇ¨ Ï≤òÎ¶¨ ÏôÑÎ£å")
                if any(kw in content for kw in selected_keywords):
                    article_list[i]['content'] = content
                    results.append(article_list[i])
        return results

    result = asyncio.run(collect())
    progress_placeholder.empty()
    return result

# ‚úÖ Ïó∞Ìï©Îâ¥Ïä§ ÏàòÏßë
def parse_yonhap():
    collected, page = [], 1
    status_placeholder.info("\U0001F50D [Ïó∞Ìï©Îâ¥Ïä§] Î™©Î°ù ÏàòÏßë Ï§ë...")
    while True:
        url = f"https://www.yna.co.kr/news/{page}?site=navi_latest_depth01"
        res = httpx.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=5.0)
        soup = BeautifulSoup(res.text, "html.parser")
        items = soup.select("ul.list01 > li[data-cid]")
        if not items:
            break
        for item in items:
            cid = item.get("data-cid")
            title_tag = item.select_one(".title01")
            time_tag = item.select_one(".txt-time")
            if not (cid and title_tag and time_tag):
                continue
            try:
                dt = datetime.strptime(f"{start_dt.year}-{time_tag.text.strip()}", "%Y-%m-%d %H:%M").replace(tzinfo=ZoneInfo("Asia/Seoul"))
            except:
                continue
            if dt < start_dt:
                return fetch_articles_concurrently(collected, "div.story-news.article")
            if start_dt <= dt <= end_dt:
                collected.append({
                    "source": "Ïó∞Ìï©Îâ¥Ïä§", "datetime": dt, "title": title_tag.text.strip(),
                    "url": f"https://www.yna.co.kr/view/{cid}"
                })
        page += 1
    return fetch_articles_concurrently(collected, "div.story-news.article")

# ‚úÖ Îâ¥ÏãúÏä§ ÏàòÏßë
def parse_newsis():
    collected, page = [], 1
    status_placeholder.info("\U0001F50D [Îâ¥ÏãúÏä§] Î™©Î°ù ÏàòÏßë Ï§ë...")
    while True:
        url = f"https://www.newsis.com/realnews/?cid=realnews&day=today&page={page}"
        res = httpx.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=5.0)
        soup = BeautifulSoup(res.text, "html.parser")
        items = soup.select("ul.articleList2 > li")
        if not items:
            break
        for item in items:
            title_tag = item.select_one("p.tit > a")
            time_tag = item.select_one("p.time")
            if not (title_tag and time_tag):
                continue
            title = title_tag.get_text(strip=True)
            href = title_tag.get("href", "")
            match = re.search(r"\d{4}\.\d{2}\.\d{2} \d{2}:\d{2}:\d{2}", time_tag.text)
            if not match:
                continue
            dt = datetime.strptime(match.group(), "%Y.%m.%d %H:%M:%S").replace(tzinfo=ZoneInfo("Asia/Seoul"))
            if dt < start_dt:
                return fetch_articles_concurrently(collected, "div.viewer")
            if start_dt <= dt <= end_dt:
                collected.append({
                    "source": "Îâ¥ÏãúÏä§", "datetime": dt, "title": title,
                    "url": "https://www.newsis.com" + href
                })
        page += 1
    return fetch_articles_concurrently(collected, "div.viewer")

# ‚úÖ ÏàòÏßë Î≤ÑÌäº ÌÅ¥Î¶≠ Ïãú Ïã§Ìñâ
if st.button("\U0001F4E5 Í∏∞ÏÇ¨ ÏàòÏßë ÏãúÏûë"):
    status_placeholder.info("Í∏∞ÏÇ¨ ÏàòÏßëÏùÑ ÏãúÏûëÌï©ÎãàÎã§...")
    newsis_articles = parse_newsis()
    yonhap_articles = parse_yonhap()
    articles = newsis_articles + yonhap_articles

    status_placeholder.success(f"‚úÖ Ï¥ù {len(articles)}Í±¥Ïùò Í∏∞ÏÇ¨Î•º ÏàòÏßëÌñàÏäµÎãàÎã§.")

    if articles:
        st.subheader("\U0001F4F0 Í∏∞ÏÇ¨ ÎÇ¥Ïö©")
        for art in articles:
            matched_kw = [kw for kw in selected_keywords if kw in art["content"]]
            st.markdown(f"**[{art['title']}]({art['url']})**")
            st.markdown(f"{art['datetime'].strftime('%Y-%m-%d %H:%M')} | ÌïÑÌÑ∞ÎßÅ ÌÇ§ÏõåÎìú: {', '.join(matched_kw)}")
            st.markdown(highlight_keywords(art['content'], matched_kw).replace("\n", "\n\n"))
            st.markdown("---")

        # üìã Î≥µÏÇ¨Ïö© ÌÖçÏä§Ìä∏ ÏÉùÏÑ±
        st.subheader("\ud83d\udccb Î≥µÏÇ¨Ïö© ÏöîÏïΩ ÌÖçÏä§Ìä∏")
        text_block = ""
        for art in articles:
            text_block += f"\u25b3{art['title']}\n-" + art["content"].replace("\n", " ").strip()[:300] + "\n\n"

        st.text_area("Î≥µÏÇ¨Ìï† ÎÇ¥Ïö©", text_block, height=300, key="copy_text")

        st.markdown(f"""
        <textarea id="copy-target" style="height:0; opacity:0">{text_block}</textarea>
        <button onclick="navigator.clipboard.writeText(document.getElementById('copy-target').value).then(()=>alert('ÌÅ¥Î¶ΩÎ≥¥ÎìúÏóê Î≥µÏÇ¨ÎêòÏóàÏäµÎãàÎã§!'))"
            style="padding:10px; font-size:16px;">\ud83d\udccb ÌÅ¥Î¶ΩÎ≥¥ÎìúÏóê Î≥µÏÇ¨</button>
        """, unsafe_allow_html=True)
