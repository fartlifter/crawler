import streamlit as st
import requests
from bs4 import BeautifulSoup
from datetime import datetime, date, time as dtime
import re
import time as t

st.set_page_config(page_title="ë‰´ìŠ¤ í‚¤ì›Œë“œ ìˆ˜ì§‘ê¸°", layout="wide")

# âœ… í‚¤ì›Œë“œ ê·¸ë£¹
keyword_groups = {
    'ì‹œê²½': ['ì„œìš¸ê²½ì°°ì²­'],
    'ë³¸ì²­': ['ê²½ì°°ì²­'],
    'ì¢…í˜œë¶': [
        'ì¢…ë¡œ', 'ì¢…ì•”', 'ì„±ë¶', 'ê³ ë ¤ëŒ€', 'ì°¸ì—¬ì—°ëŒ€', 'í˜œí™”', 'ë™ëŒ€ë¬¸', 'ì¤‘ëž‘',
        'ì„±ê· ê´€ëŒ€', 'í•œêµ­ì™¸ëŒ€', 'ì„œìš¸ì‹œë¦½ëŒ€', 'ê²½í¬ëŒ€', 'ê²½ì‹¤ë ¨', 'ì„œìš¸ëŒ€ë³‘ì›',
        'ë…¸ì›', 'ê°•ë¶', 'ë„ë´‰', 'ë¶ë¶€ì§€ë²•', 'ë¶ë¶€ì§€ê²€', 'ìƒê³„ë°±ë³‘ì›', 'êµ­ê°€ì¸ê¶Œìœ„ì›íšŒ'
    ],
    'ë§ˆí¬ì¤‘ë¶€': [
        'ë§ˆí¬', 'ì„œëŒ€ë¬¸', 'ì„œë¶€', 'ì€í‰', 'ì„œë¶€ì§€ê²€', 'ì„œë¶€ì§€ë²•', 'ì—°ì„¸ëŒ€',
        'ì‹ ì´Œì„¸ë¸Œëž€ìŠ¤ë³‘ì›', 'êµ°ì¸ê¶Œì„¼í„°', 'ì¤‘ë¶€', 'ë‚¨ëŒ€ë¬¸', 'ìš©ì‚°', 'ë™êµ­ëŒ€',
        'ìˆ™ëª…ì—¬ëŒ€', 'ìˆœì²œí–¥ëŒ€ë³‘ì›'
    ],
    'ì˜ë“±í¬ê´€ì•…': [
        'ì˜ë“±í¬', 'ì–‘ì²œ', 'êµ¬ë¡œ', 'ê°•ì„œ', 'ë‚¨ë¶€ì§€ê²€', 'ë‚¨ë¶€ì§€ë²•', 'ì—¬ì˜ë„ì„±ëª¨ë³‘ì›',
        'ê³ ëŒ€êµ¬ë¡œë³‘ì›', 'ê´€ì•…', 'ê¸ˆì²œ', 'ë™ìž‘', 'ë°©ë°°', 'ì„œìš¸ëŒ€', 'ì¤‘ì•™ëŒ€', 'ìˆ­ì‹¤ëŒ€', 'ë³´ë¼ë§¤ë³‘ì›'
    ],
    'ê°•ë‚¨ê´‘ì§„': [
        'ê°•ë‚¨', 'ì„œì´ˆ', 'ìˆ˜ì„œ', 'ì†¡íŒŒ', 'ê°•ë™', 'ì‚¼ì„±ì˜ë£Œì›', 'í˜„ëŒ€ì•„ì‚°ë³‘ì›',
        'ê°•ë‚¨ì„¸ë¸Œëž€ìŠ¤ë³‘ì›', 'ê´‘ì§„', 'ì„±ë™', 'ë™ë¶€ì§€ê²€', 'ë™ë¶€ì§€ë²•', 'í•œì–‘ëŒ€',
        'ê±´êµ­ëŒ€', 'ì„¸ì¢…ëŒ€'
    ]
}

# âœ… ì‚¬ìš©ìž ìž…ë ¥
st.title("ðŸ“° ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ (ì—°í•©ë‰´ìŠ¤ + ë‰´ì‹œìŠ¤)")
col1, col2 = st.columns(2)
with col1:
    start_date = st.date_input("ì‹œìž‘ ë‚ ì§œ", value=date.today())
    start_time = st.time_input("ì‹œìž‘ ì‹œê°„", value=dtime(18, 0))
with col2:
    end_date = st.date_input("ì¢…ë£Œ ë‚ ì§œ", value=date.today())
    end_time = st.time_input("ì¢…ë£Œ ì‹œê°„", value=dtime(22, 0))

selected_groups = st.multiselect("í‚¤ì›Œë“œ ê·¸ë£¹ ì„ íƒ", options=list(keyword_groups.keys()), default=['ì‹œê²½', 'ì¢…í˜œë¶'])

start_dt = datetime.combine(start_date, start_time)
end_dt = datetime.combine(end_date, end_time)
selected_keywords = [kw for g in selected_groups for kw in keyword_groups[g]]

if st.button("ðŸ“¥ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹œìž‘"):
    st.info("ê¸°ì‚¬ ìˆ˜ì§‘ì„ ì‹œìž‘í•©ë‹ˆë‹¤...")

    def highlight_keywords(text, keywords):
        for kw in keywords:
            text = re.sub(f"({re.escape(kw)})", r"**\1**", text)
        return text

    def get_newsis_content(url):
        try:
            res = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})
            soup = BeautifulSoup(res.text, "html.parser")
            content = soup.find("div", class_="viewer")
            return content.get_text(separator="\n", strip=True) if content else ""
        except:
            return ""

    def get_yonhap_content(url):
        try:
            res = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})
            soup = BeautifulSoup(res.text, "html.parser")
            content = soup.find("div", class_="story-news article")
            return content.get_text(separator="\n", strip=True) if content else ""
        except:
            return ""

    def parse_newsis():
        results, page = [], 1
        st.write("ðŸ” [ë‰´ì‹œìŠ¤] ìˆ˜ì§‘ ì‹œìž‘")
        while True:
            url = f"https://www.newsis.com/realnews/?cid=realnews&day=today&page={page}"
            res = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})
            soup = BeautifulSoup(res.text, "html.parser")
            items = soup.select("ul.articleList2 > li")
            if not items:
                break
            for item in items:
                title_tag = item.select_one("p.tit > a")
                time_tag = item.select_one("p.time")
                if not (title_tag and time_tag):
                    continue
                title = title_tag.get_text(strip=True)
                href = title_tag.get("href", "")
                if not href.startswith("/view/"):
                    continue
                url_full = "https://www.newsis.com" + href
                match = re.search(r"\d{4}\.\d{2}\.\d{2} \d{2}:\d{2}:\d{2}", time_tag.text)
                if not match:
                    continue
                dt = datetime.strptime(match.group(), "%Y.%m.%d %H:%M:%S")
                if dt < start_dt:
                    return results
                if start_dt <= dt <= end_dt:
                    content = get_newsis_content(url_full)
                    if any(kw in content for kw in selected_keywords):
                        results.append({
                            "source": "ë‰´ì‹œìŠ¤",
                            "datetime": dt,
                            "title": title,
                            "url": url_full,
                            "content": content
                        })
            page += 1
            t.sleep(0.8)
        return results

    def parse_yonhap():
        results, page = [], 1
        st.write("ðŸ” [ì—°í•©ë‰´ìŠ¤] ìˆ˜ì§‘ ì‹œìž‘")
        while True:
            url = f"https://www.yna.co.kr/news/{page}?site=navi_latest_depth01"
            res = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})
            soup = BeautifulSoup(res.text, "html.parser")
            items = soup.select("ul.list01 > li[data-cid]")
            if not items:
                break
            for item in items:
                cid = item.get("data-cid")
                title_tag = item.select_one(".title01")
                time_tag = item.select_one(".txt-time")
                if not (cid and title_tag and time_tag):
                    continue
                dt_str = time_tag.get_text(strip=True)
                try:
                    dt = datetime.strptime(f"{start_dt.year}-{dt_str}", "%Y-%m-%d %H:%M")
                except:
                    continue
                if dt < start_dt:
                    return results
                if start_dt <= dt <= end_dt:
                    url_full = f"https://www.yna.co.kr/view/{cid}"
                    content = get_yonhap_content(url_full)
                    if any(kw in content for kw in selected_keywords):
                        results.append({
                            "source": "ì—°í•©ë‰´ìŠ¤",
                            "datetime": dt,
                            "title": title_tag.text.strip(),
                            "url": url_full,
                            "content": content
                        })
            page += 1
            t.sleep(0.8)
        return results

    newsis_articles = parse_newsis()
    yonhap_articles = parse_yonhap()
    articles = newsis_articles + yonhap_articles

    st.success(f"âœ… ì´ {len(articles)}ê±´ì˜ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")

    if articles:
        st.subheader("ðŸ“° ê¸°ì‚¬ ë‚´ìš©")
        for art in articles:
            matched_kw = [kw for kw in selected_keywords if kw in art["content"]]
            st.markdown(f"**[{art['title']}]({art['url']})**")
            st.markdown(f"{art['datetime'].strftime('%Y-%m-%d %H:%M')} | í•„í„°ë§ í‚¤ì›Œë“œ: {', '.join(matched_kw)}")
            st.markdown(highlight_keywords(art['content'], matched_kw).replace("\n", "\n\n"))
            st.markdown("---")

        st.subheader("ðŸ“‹ ë³µì‚¬ìš© ìš”ì•½ í…ìŠ¤íŠ¸")
        text_block = ""
        for art in articles:
            matched_kw = [kw for kw in selected_keywords if kw in art["content"]]
            text_block += f"â–³{art['title']}\n-" + art["content"].replace("\n", " ").strip()[:300] + "\n\n"
        st.text_area("ë³µì‚¬í•˜ì„¸ìš”", text_block, height=300)
